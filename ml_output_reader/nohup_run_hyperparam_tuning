nohup: ignoring input
2022-03-07 13:05:17.990162: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-03-07 13:05:19.581780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-03-07 13:05:19.626612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:17:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s
2022-03-07 13:05:19.626690: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-03-07 13:05:19.630784: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-03-07 13:05:19.630885: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-03-07 13:05:19.631735: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-03-07 13:05:19.632026: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-03-07 13:05:19.632476: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-03-07 13:05:19.633213: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-03-07 13:05:19.633387: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-03-07 13:05:19.635111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2022-03-07 13:05:19.635432: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-03-07 13:05:19.637627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:17:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s
2022-03-07 13:05:19.639064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2022-03-07 13:05:19.639099: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-03-07 13:05:20.012687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-03-07 13:05:20.012729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2022-03-07 13:05:20.012736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2022-03-07 13:05:20.014479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21501 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6)
2022-03-07 13:05:20.285384: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-03-07 13:05:20.861596: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-03-07 13:05:20.861682: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertModel.call of <transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x7fd4e0fc0130>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
type(data[0]) = <class 'str'>
89004
(89004,)
Classifier with lr = 1e-06,  dropout_rate = 0.0
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x7fd4e1806ca0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x7fd4e1806e80>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertWordEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertWordEmbeddings object at 0x7fd4e1806f70>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:From /home/sam/projects/virtual_env_dl/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.
Instructions for updating:
The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertPositionEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertPositionEmbeddings object at 0x7fd4e0fc0070>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertTokenTypeEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertTokenTypeEmbeddings object at 0x7fd4e0fc0c10>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x7fd4e0fc0910>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x7fd4e0fc0e80>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x7fd4e0fc0fd0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x7fd4e032f130>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfOutput object at 0x7fd4e032ff40>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.models.bert.modeling_tf_bert.TFBertIntermediate object at 0x7fd4e033f8e0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.models.bert.modeling_tf_bert.TFBertOutput object at 0x7fd4e033fee0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x7fd4e0fc0e50>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-03-07 13:05:22.035899: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-03-07 13:05:22.053978: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3499910000 Hz
/home/sam/projects/virtual_env_dl/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  warnings.warn(
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd4d0680af0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <bound method FBetaScore.update_state of <tensorflow_addons.metrics.f_scores.F1Score object at 0x7fd4e0010be0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: invalid value for "node": expected "ast.AST", got "<class 'NoneType'>"; to visit lists of nodes, use "visit_block" instead
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method FBetaScore.result of <tensorflow_addons.metrics.f_scores.F1Score object at 0x7fd4e0010be0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
/home/sam/projects/virtual_env_dl/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py:254: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  warnings.warn('Metric %s implements a `reset_states()` method; rename it '
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc097cdc0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.1156 - f1_score: 0.0053 - precision: 0.0030 - recall: 0.0368 - val_loss: 0.0187 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 2/100
357/357 - 160s - loss: 0.0020 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0205 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 3/100
357/357 - 161s - loss: 7.2556e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0222 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 4/100
357/357 - 161s - loss: 3.9914e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0235 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 2.7466e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0245 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 2.0764e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0254 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 1.6820e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0262 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 8/100
357/357 - 161s - loss: 1.4484e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0268 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 9/100
357/357 - 161s - loss: 1.3004e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0274 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 10/100
357/357 - 162s - loss: 1.1922e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0280 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 11/100
357/357 - 161s - loss: 1.1210e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0285 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 12/100
357/357 - 161s - loss: 1.0601e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0290 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 13/100
357/357 - 161s - loss: 1.0281e-04 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0294 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 14/100
357/357 - 160s - loss: 9.9626e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0298 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 15/100
357/357 - 161s - loss: 9.7320e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0302 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 16/100
357/357 - 160s - loss: 9.6414e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0305 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 17/100
357/357 - 161s - loss: 9.5364e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0308 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 18/100
357/357 - 161s - loss: 9.4709e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0310 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 19/100
357/357 - 161s - loss: 9.4011e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0312 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 20/100
357/357 - 161s - loss: 9.3060e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0313 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 21/100
357/357 - 160s - loss: 9.3077e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0314 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 22/100
357/357 - 161s - loss: 9.2783e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0315 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 23/100
357/357 - 160s - loss: 9.2475e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0315 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 24/100
357/357 - 161s - loss: 9.2286e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0314 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 25/100
357/357 - 160s - loss: 9.1963e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0314 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 26/100
357/357 - 161s - loss: 9.1687e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0313 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 27/100
357/357 - 160s - loss: 9.0889e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0312 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 28/100
357/357 - 160s - loss: 9.0480e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0310 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 29/100
357/357 - 160s - loss: 8.9893e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0307 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 30/100
357/357 - 160s - loss: 8.9187e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0306 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 31/100
357/357 - 161s - loss: 8.8253e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0298 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 32/100
357/357 - 160s - loss: 8.7092e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0298 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 33/100
357/357 - 160s - loss: 8.4881e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0291 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 34/100
357/357 - 160s - loss: 8.2475e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0274 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 35/100
357/357 - 160s - loss: 7.8264e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0276 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 36/100
357/357 - 160s - loss: 7.3375e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0255 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 37/100
357/357 - 160s - loss: 6.9785e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0238 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 38/100
357/357 - 160s - loss: 6.6068e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0221 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 39/100
357/357 - 160s - loss: 6.1329e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0180 - val_f1_score: 0.0049 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
Epoch 40/100
357/357 - 160s - loss: 5.5257e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0183 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.0227
Epoch 41/100
357/357 - 160s - loss: 5.3335e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.0158 - val_loss: 0.0191 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.0227
Epoch 42/100
357/357 - 160s - loss: 5.2512e-05 - f1_score: 0.0053 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0170 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.0227
Epoch 43/100
357/357 - 160s - loss: 4.2976e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.0632 - val_loss: 0.0172 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.0682
Epoch 44/100
357/357 - 160s - loss: 4.2972e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.1368 - val_loss: 0.0210 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.0455
Epoch 45/100
357/357 - 161s - loss: 3.7462e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.2053 - val_loss: 0.0173 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.2500
Epoch 46/100
357/357 - 162s - loss: 3.4544e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.2842 - val_loss: 0.0226 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.0682
Epoch 47/100
357/357 - 162s - loss: 4.4797e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.1895 - val_loss: 0.0149 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.2500
Epoch 48/100
357/357 - 161s - loss: 2.8504e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.3895 - val_loss: 0.0146 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.2955
Epoch 49/100
357/357 - 164s - loss: 3.0391e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.4421 - val_loss: 0.0174 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.2955
Epoch 50/100
357/357 - 161s - loss: 2.8369e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.4263 - val_loss: 0.0140 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.3636
Epoch 51/100
357/357 - 163s - loss: 2.5663e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.5105 - val_loss: 0.0160 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.3409
Epoch 52/100
357/357 - 162s - loss: 2.1918e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.5421 - val_loss: 0.0160 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.3409
Epoch 53/100
357/357 - 162s - loss: 2.0714e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.5947 - val_loss: 0.0126 - val_f1_score: 0.0049 - val_precision: 0.9474 - val_recall: 0.4091
Epoch 54/100
357/357 - 162s - loss: 3.6620e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.3895 - val_loss: 0.0201 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.2500
Epoch 55/100
357/357 - 162s - loss: 2.3329e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.5211 - val_loss: 0.0138 - val_f1_score: 0.0049 - val_precision: 0.9444 - val_recall: 0.3864
Epoch 56/100
357/357 - 161s - loss: 2.1952e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.5684 - val_loss: 0.0120 - val_f1_score: 0.0049 - val_precision: 0.9412 - val_recall: 0.3636
Epoch 57/100
357/357 - 161s - loss: 1.8150e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.6474 - val_loss: 0.0131 - val_f1_score: 0.0049 - val_precision: 0.9412 - val_recall: 0.3636
Epoch 58/100
357/357 - 160s - loss: 1.6468e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.6789 - val_loss: 0.0125 - val_f1_score: 0.0049 - val_precision: 0.8889 - val_recall: 0.3636
Epoch 59/100
357/357 - 160s - loss: 1.8763e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.6632 - val_loss: 0.0146 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.3636
Epoch 60/100
357/357 - 161s - loss: 1.4011e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.7158 - val_loss: 0.0124 - val_f1_score: 0.0049 - val_precision: 0.9000 - val_recall: 0.4091
Epoch 61/100
357/357 - 164s - loss: 1.3595e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.7316 - val_loss: 0.0108 - val_f1_score: 0.0049 - val_precision: 0.8846 - val_recall: 0.5227
Epoch 62/100
357/357 - 163s - loss: 1.8474e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.6684 - val_loss: 0.0159 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.3636
Epoch 63/100
357/357 - 163s - loss: 1.2998e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.7526 - val_loss: 0.0141 - val_f1_score: 0.0049 - val_precision: 0.9474 - val_recall: 0.4091
Epoch 64/100
357/357 - 161s - loss: 1.6820e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.6684 - val_loss: 0.0128 - val_f1_score: 0.0049 - val_precision: 0.8947 - val_recall: 0.3864
Epoch 65/100
357/357 - 163s - loss: 4.2364e-05 - f1_score: 0.0053 - precision: 0.9886 - recall: 0.4579 - val_loss: 0.0166 - val_f1_score: 0.0049 - val_precision: 0.9375 - val_recall: 0.3409
Epoch 66/100
357/357 - 162s - loss: 1.2976e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.7211 - val_loss: 0.0123 - val_f1_score: 0.0049 - val_precision: 0.8636 - val_recall: 0.4318
Epoch 67/100
357/357 - 160s - loss: 1.3821e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.7263 - val_loss: 0.0139 - val_f1_score: 0.0049 - val_precision: 0.9444 - val_recall: 0.3864
Epoch 68/100
357/357 - 161s - loss: 1.1991e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.7789 - val_loss: 0.0141 - val_f1_score: 0.0049 - val_precision: 0.9444 - val_recall: 0.3864
Epoch 69/100
357/357 - 161s - loss: 1.0015e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.8211 - val_loss: 0.0130 - val_f1_score: 0.0049 - val_precision: 0.8947 - val_recall: 0.3864
Epoch 70/100
357/357 - 161s - loss: 9.6349e-06 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.8316 - val_loss: 0.0124 - val_f1_score: 0.0049 - val_precision: 0.8571 - val_recall: 0.4091
Epoch 71/100
357/357 - 161s - loss: 1.0496e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.7895 - val_loss: 0.0145 - val_f1_score: 0.0049 - val_precision: 0.9444 - val_recall: 0.3864
Epoch 72/100
357/357 - 160s - loss: 9.6651e-06 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.8158 - val_loss: 0.0123 - val_f1_score: 0.0049 - val_precision: 0.8696 - val_recall: 0.4545
Epoch 73/100
357/357 - 161s - loss: 1.0410e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.8053 - val_loss: 0.0119 - val_f1_score: 0.0049 - val_precision: 0.9167 - val_recall: 0.5000
Epoch 74/100
357/357 - 161s - loss: 7.4975e-06 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.8421 - val_loss: 0.0110 - val_f1_score: 0.0049 - val_precision: 0.8846 - val_recall: 0.5227
Epoch 75/100
357/357 - 161s - loss: 9.6239e-06 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.8158 - val_loss: 0.0178 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.3409
Epoch 76/100
357/357 - 160s - loss: 7.7174e-06 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.8368 - val_loss: 0.0113 - val_f1_score: 0.0049 - val_precision: 0.9200 - val_recall: 0.5227
Epoch 77/100
357/357 - 160s - loss: 2.8105e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.5737 - val_loss: 0.0206 - val_f1_score: 0.0049 - val_precision: 1.0000 - val_recall: 0.3182
Epoch 78/100
357/357 - 160s - loss: 1.2786e-05 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.7000 - val_loss: 0.0124 - val_f1_score: 0.0049 - val_precision: 0.9048 - val_recall: 0.4318
Epoch 79/100
357/357 - 161s - loss: 7.6102e-06 - f1_score: 0.0053 - precision: 1.0000 - recall: 0.8579 - val_loss: 0.0117 - val_f1_score: 0.0049 - val_precision: 0.9091 - val_recall: 0.4545
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc022e310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 1e-06,  dropout_rate = 0.4
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc2e2f550> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 169s - loss: 0.1119 - f1_score: 0.0053 - precision_1: 0.0026 - recall_1: 0.0474 - val_loss: 0.0190 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 2/100
357/357 - 161s - loss: 0.0032 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0217 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 3/100
357/357 - 162s - loss: 0.0013 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0240 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 4/100
357/357 - 162s - loss: 7.1504e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0258 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 5/100
357/357 - 163s - loss: 4.7198e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0273 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 6/100
357/357 - 161s - loss: 3.5250e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0286 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 7/100
357/357 - 161s - loss: 2.6675e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0297 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 8/100
357/357 - 161s - loss: 2.1661e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0307 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 1.9291e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0316 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 10/100
357/357 - 160s - loss: 1.6504e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0324 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 11/100
357/357 - 161s - loss: 1.5416e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0331 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 12/100
357/357 - 160s - loss: 1.4365e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0338 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 13/100
357/357 - 160s - loss: 1.3584e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0344 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 14/100
357/357 - 160s - loss: 1.3090e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0349 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 15/100
357/357 - 160s - loss: 1.2505e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0353 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 16/100
357/357 - 160s - loss: 1.2171e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0357 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 17/100
357/357 - 160s - loss: 1.1714e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0361 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 18/100
357/357 - 160s - loss: 1.2032e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0363 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 19/100
357/357 - 160s - loss: 1.1517e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0365 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 20/100
357/357 - 160s - loss: 1.1410e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0367 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 21/100
357/357 - 160s - loss: 1.1509e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0367 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 22/100
357/357 - 160s - loss: 1.1454e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0367 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 23/100
357/357 - 160s - loss: 1.1360e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0366 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 24/100
357/357 - 161s - loss: 1.1188e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0364 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 25/100
357/357 - 160s - loss: 1.1089e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0362 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 26/100
357/357 - 160s - loss: 1.1005e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0361 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 27/100
357/357 - 160s - loss: 1.0990e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0359 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 28/100
357/357 - 160s - loss: 1.0713e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0355 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 29/100
357/357 - 160s - loss: 1.0704e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0350 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 30/100
357/357 - 160s - loss: 1.0375e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0351 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 31/100
357/357 - 160s - loss: 1.0538e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0340 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 32/100
357/357 - 161s - loss: 1.0213e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0341 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 33/100
357/357 - 160s - loss: 1.0199e-04 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0328 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 34/100
357/357 - 160s - loss: 9.6680e-05 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0307 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 35/100
357/357 - 160s - loss: 9.2887e-05 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0280 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 36/100
357/357 - 160s - loss: 8.7395e-05 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0264 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 37/100
357/357 - 160s - loss: 8.0866e-05 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0246 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 38/100
357/357 - 160s - loss: 7.6161e-05 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0240 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 39/100
357/357 - 160s - loss: 7.0415e-05 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0212 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 40/100
357/357 - 160s - loss: 6.2938e-05 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0207 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 41/100
357/357 - 160s - loss: 6.1301e-05 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0218 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 42/100
357/357 - 160s - loss: 5.5809e-05 - f1_score: 0.0053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.0171 - val_f1_score: 0.0049 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00
Epoch 43/100
357/357 - 159s - loss: 4.7290e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.0211 - val_loss: 0.0170 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.0227
Epoch 44/100
357/357 - 160s - loss: 3.7953e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.0947 - val_loss: 0.0160 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.2045
Epoch 45/100
357/357 - 160s - loss: 4.0742e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.1263 - val_loss: 0.0211 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.0909
Epoch 46/100
357/357 - 160s - loss: 3.1884e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.3263 - val_loss: 0.0137 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.2727
Epoch 47/100
357/357 - 160s - loss: 3.3753e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.3316 - val_loss: 0.0199 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.2273
Epoch 48/100
357/357 - 160s - loss: 3.3413e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.3579 - val_loss: 0.0232 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.1591
Epoch 49/100
357/357 - 160s - loss: 2.9452e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.3105 - val_loss: 0.0133 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.2955
Epoch 50/100
357/357 - 160s - loss: 3.4450e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.3684 - val_loss: 0.0169 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.2727
Epoch 51/100
357/357 - 160s - loss: 3.7758e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.2579 - val_loss: 0.0130 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.2955
Epoch 52/100
357/357 - 160s - loss: 2.3282e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.4000 - val_loss: 0.0126 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.3182
Epoch 53/100
357/357 - 161s - loss: 3.1084e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.3842 - val_loss: 0.0178 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.2273
Epoch 54/100
357/357 - 161s - loss: 2.3151e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.3579 - val_loss: 0.0127 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.3182
Epoch 55/100
357/357 - 161s - loss: 2.4792e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.4105 - val_loss: 0.0123 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.3409
Epoch 56/100
357/357 - 160s - loss: 1.8912e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.5421 - val_loss: 0.0112 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.4091
Epoch 57/100
357/357 - 160s - loss: 1.9178e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.5895 - val_loss: 0.0123 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.4091
Epoch 58/100
357/357 - 160s - loss: 2.1108e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.5842 - val_loss: 0.0219 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.2500
Epoch 59/100
357/357 - 160s - loss: 1.9677e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.5526 - val_loss: 0.0173 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.3182
Epoch 60/100
357/357 - 160s - loss: 1.4761e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.6211 - val_loss: 0.0117 - val_f1_score: 0.0049 - val_precision_1: 0.9091 - val_recall_1: 0.4545
Epoch 61/100
357/357 - 160s - loss: 1.6931e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.6158 - val_loss: 0.0136 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.4318
Epoch 62/100
357/357 - 161s - loss: 1.4273e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.7105 - val_loss: 0.0143 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.4318
Epoch 63/100
357/357 - 160s - loss: 9.5929e-06 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.7789 - val_loss: 0.0114 - val_f1_score: 0.0049 - val_precision_1: 0.9231 - val_recall_1: 0.5455
Epoch 64/100
357/357 - 160s - loss: 1.0323e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.8211 - val_loss: 0.0137 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.4773
Epoch 65/100
357/357 - 160s - loss: 8.9265e-06 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.8421 - val_loss: 0.0117 - val_f1_score: 0.0049 - val_precision_1: 0.9259 - val_recall_1: 0.5682
Epoch 66/100
357/357 - 160s - loss: 7.7017e-06 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.8368 - val_loss: 0.0111 - val_f1_score: 0.0049 - val_precision_1: 0.9259 - val_recall_1: 0.5682
Epoch 67/100
357/357 - 160s - loss: 1.4556e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.7368 - val_loss: 0.0124 - val_f1_score: 0.0049 - val_precision_1: 0.9200 - val_recall_1: 0.5227
Epoch 68/100
357/357 - 160s - loss: 7.7873e-06 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.8632 - val_loss: 0.0121 - val_f1_score: 0.0049 - val_precision_1: 0.9200 - val_recall_1: 0.5227
Epoch 69/100
357/357 - 160s - loss: 8.0395e-06 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.8684 - val_loss: 0.0141 - val_f1_score: 0.0049 - val_precision_1: 0.9565 - val_recall_1: 0.5000
Epoch 70/100
357/357 - 161s - loss: 2.3056e-05 - f1_score: 0.0053 - precision_1: 0.9940 - recall_1: 0.8789 - val_loss: 0.0280 - val_f1_score: 0.0049 - val_precision_1: 1.0000 - val_recall_1: 0.1364
Epoch 71/100
357/357 - 160s - loss: 1.5083e-05 - f1_score: 0.0053 - precision_1: 1.0000 - recall_1: 0.6263 - val_loss: 0.0145 - val_f1_score: 0.0049 - val_precision_1: 0.9545 - val_recall_1: 0.4773
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd38c0cdca0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 1e-06,  dropout_rate = 0.8
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd38c058550> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.1179 - f1_score: 0.0053 - precision_2: 0.0020 - recall_2: 0.0316 - val_loss: 0.0231 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 2/100
357/357 - 160s - loss: 0.0093 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 3/100
357/357 - 161s - loss: 0.0058 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0327 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 4/100
357/357 - 161s - loss: 0.0044 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0353 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 0.0039 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0372 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 0.0033 - f1_score: 0.0052 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0389 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 0.0028 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0403 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 8/100
357/357 - 160s - loss: 0.0026 - f1_score: 0.0052 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0416 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 0.0026 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0425 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 10/100
357/357 - 160s - loss: 0.0024 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0435 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 11/100
357/357 - 160s - loss: 0.0022 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0444 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 12/100
357/357 - 160s - loss: 0.0024 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0450 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 13/100
357/357 - 160s - loss: 0.0019 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0458 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 14/100
357/357 - 160s - loss: 0.0017 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0462 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 15/100
357/357 - 161s - loss: 0.0019 - f1_score: 0.0052 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0467 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 16/100
357/357 - 160s - loss: 0.0018 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0472 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 17/100
357/357 - 160s - loss: 0.0017 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0476 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 18/100
357/357 - 161s - loss: 0.0016 - f1_score: 0.0051 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0480 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 19/100
357/357 - 160s - loss: 0.0017 - f1_score: 0.0052 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0482 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 20/100
357/357 - 160s - loss: 0.0015 - f1_score: 0.0051 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0490 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 21/100
357/357 - 161s - loss: 0.0015 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0493 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 22/100
357/357 - 160s - loss: 0.0015 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0497 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 23/100
357/357 - 160s - loss: 0.0016 - f1_score: 0.0050 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0501 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 24/100
357/357 - 160s - loss: 0.0014 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0505 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 25/100
357/357 - 161s - loss: 0.0013 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0509 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 26/100
357/357 - 161s - loss: 0.0014 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0514 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 27/100
357/357 - 160s - loss: 0.0014 - f1_score: 0.0050 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0514 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 28/100
357/357 - 160s - loss: 0.0013 - f1_score: 0.0055 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0512 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 29/100
357/357 - 160s - loss: 0.0013 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0517 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 30/100
357/357 - 160s - loss: 0.0012 - f1_score: 0.0055 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0516 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 31/100
357/357 - 160s - loss: 0.0013 - f1_score: 0.0055 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0521 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 32/100
357/357 - 160s - loss: 0.0014 - f1_score: 0.0055 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0527 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 33/100
357/357 - 161s - loss: 0.0012 - f1_score: 0.0055 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0530 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 34/100
357/357 - 160s - loss: 0.0012 - f1_score: 0.0051 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0531 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 35/100
357/357 - 160s - loss: 0.0011 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0535 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 36/100
357/357 - 160s - loss: 0.0012 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0536 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 37/100
357/357 - 161s - loss: 0.0012 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0538 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 38/100
357/357 - 161s - loss: 0.0012 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0542 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 39/100
357/357 - 160s - loss: 0.0011 - f1_score: 0.0055 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0543 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 40/100
357/357 - 160s - loss: 0.0011 - f1_score: 0.0052 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0543 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 41/100
357/357 - 160s - loss: 0.0011 - f1_score: 0.0048 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0550 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 42/100
357/357 - 161s - loss: 0.0012 - f1_score: 0.0052 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0547 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 43/100
357/357 - 161s - loss: 0.0010 - f1_score: 0.0052 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0553 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 44/100
357/357 - 161s - loss: 9.8866e-04 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0554 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 45/100
357/357 - 160s - loss: 0.0011 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0555 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 46/100
357/357 - 160s - loss: 0.0012 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0555 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 47/100
357/357 - 160s - loss: 9.5458e-04 - f1_score: 0.0056 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0555 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 48/100
357/357 - 160s - loss: 0.0010 - f1_score: 0.0050 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0560 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 49/100
357/357 - 160s - loss: 0.0010 - f1_score: 0.0056 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0560 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 50/100
357/357 - 160s - loss: 9.5559e-04 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0564 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 51/100
357/357 - 160s - loss: 9.9379e-04 - f1_score: 0.0056 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0563 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 52/100
357/357 - 160s - loss: 9.4570e-04 - f1_score: 0.0052 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0568 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 53/100
357/357 - 160s - loss: 0.0012 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0566 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 54/100
357/357 - 161s - loss: 8.7295e-04 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0575 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 55/100
357/357 - 161s - loss: 9.2170e-04 - f1_score: 0.0053 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0572 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 56/100
357/357 - 160s - loss: 9.8105e-04 - f1_score: 0.0055 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0576 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 57/100
357/357 - 160s - loss: 9.9169e-04 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0579 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 58/100
357/357 - 161s - loss: 9.5989e-04 - f1_score: 0.0054 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0574 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Epoch 59/100
357/357 - 160s - loss: 0.0010 - f1_score: 0.0050 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.0579 - val_f1_score: 0.0049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc0538ca0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 1e-05,  dropout_rate = 0.0
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd448039790> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0048 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 2/100
357/357 - 160s - loss: 9.8500e-05 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0310 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 3/100
357/357 - 160s - loss: 9.4632e-05 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0314 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 4/100
357/357 - 160s - loss: 9.2871e-05 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0314 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 5/100
357/357 - 161s - loss: 9.1441e-05 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0308 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 9.0427e-05 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0304 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 7/100
357/357 - 161s - loss: 8.8031e-05 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 8/100
357/357 - 160s - loss: 8.3061e-05 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0251 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 9/100
357/357 - 161s - loss: 6.9049e-05 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0199 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 10/100
357/357 - 160s - loss: 5.6199e-05 - f1_score: 0.0053 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - val_loss: 0.0179 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 11/100
357/357 - 161s - loss: 5.7410e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.0053 - val_loss: 0.0251 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 12/100
357/357 - 160s - loss: 5.2999e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.0053 - val_loss: 0.0125 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 13/100
357/357 - 160s - loss: 3.3601e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.0263 - val_loss: 0.0101 - val_f1_score: 0.0049 - val_precision_3: 1.0000 - val_recall_3: 0.2045
Epoch 14/100
357/357 - 160s - loss: 3.8650e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.1421 - val_loss: 0.0084 - val_f1_score: 0.0049 - val_precision_3: 1.0000 - val_recall_3: 0.2955
Epoch 15/100
357/357 - 160s - loss: 2.5056e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.3684 - val_loss: 0.0082 - val_f1_score: 0.0049 - val_precision_3: 0.9565 - val_recall_3: 0.5000
Epoch 16/100
357/357 - 161s - loss: 2.9052e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.3579 - val_loss: 0.0071 - val_f1_score: 0.0049 - val_precision_3: 0.9615 - val_recall_3: 0.5682
Epoch 17/100
357/357 - 160s - loss: 1.8804e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.6789 - val_loss: 0.0247 - val_f1_score: 0.0049 - val_precision_3: 1.0000 - val_recall_3: 0.0682
Epoch 18/100
357/357 - 160s - loss: 3.3667e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.4526 - val_loss: 0.0169 - val_f1_score: 0.0049 - val_precision_3: 1.0000 - val_recall_3: 0.2045
Epoch 19/100
357/357 - 160s - loss: 2.4187e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.4737 - val_loss: 0.0096 - val_f1_score: 0.0049 - val_precision_3: 1.0000 - val_recall_3: 0.5227
Epoch 20/100
357/357 - 160s - loss: 7.2791e-06 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.8368 - val_loss: 0.0080 - val_f1_score: 0.0049 - val_precision_3: 0.9062 - val_recall_3: 0.6591
Epoch 21/100
357/357 - 160s - loss: 2.6017e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.7368 - val_loss: 0.0286 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Epoch 22/100
357/357 - 161s - loss: 4.9954e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.1579 - val_loss: 0.0080 - val_f1_score: 0.0049 - val_precision_3: 0.9583 - val_recall_3: 0.5227
Epoch 23/100
357/357 - 161s - loss: 8.5293e-06 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.8000 - val_loss: 0.0075 - val_f1_score: 0.0049 - val_precision_3: 0.9091 - val_recall_3: 0.6818
Epoch 24/100
357/357 - 160s - loss: 1.6331e-05 - f1_score: 0.0053 - precision_3: 1.0000 - recall_3: 0.7211 - val_loss: 0.0129 - val_f1_score: 0.0049 - val_precision_3: 1.0000 - val_recall_3: 0.3636
Epoch 25/100
357/357 - 160s - loss: 3.0682e-05 - f1_score: 0.0053 - precision_3: 0.9931 - recall_3: 0.7579 - val_loss: 0.0394 - val_f1_score: 0.0049 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd97757f70> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 1e-05,  dropout_rate = 0.4
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc02a8310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0079 - f1_score: 0.0053 - precision_4: 0.0103 - recall_4: 0.0105 - val_loss: 0.0323 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 2/100
357/357 - 160s - loss: 1.5316e-04 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0356 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 3/100
357/357 - 161s - loss: 1.2890e-04 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0373 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 4/100
357/357 - 161s - loss: 1.2052e-04 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0380 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 1.1692e-04 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0380 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 1.1898e-04 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0377 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 1.1607e-04 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0367 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 8/100
357/357 - 161s - loss: 1.1312e-04 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0356 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 1.0458e-04 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0318 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 10/100
357/357 - 160s - loss: 9.3662e-05 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0242 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 11/100
357/357 - 160s - loss: 7.7663e-05 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0188 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 12/100
357/357 - 161s - loss: 7.2632e-05 - f1_score: 0.0053 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 0.0250 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 13/100
357/357 - 161s - loss: 5.6172e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.0053 - val_loss: 0.0208 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 14/100
357/357 - 161s - loss: 3.9809e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.0632 - val_loss: 0.0106 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.2955
Epoch 15/100
357/357 - 161s - loss: 7.6458e-05 - f1_score: 0.0053 - precision_4: 0.9714 - recall_4: 0.1789 - val_loss: 0.0146 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 16/100
357/357 - 161s - loss: 4.0864e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.0316 - val_loss: 0.0102 - val_f1_score: 0.0049 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00
Epoch 17/100
357/357 - 161s - loss: 4.0316e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.1105 - val_loss: 0.0087 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.3182
Epoch 18/100
357/357 - 160s - loss: 3.5230e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.2895 - val_loss: 0.0140 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.2045
Epoch 19/100
357/357 - 160s - loss: 3.2837e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.3368 - val_loss: 0.0113 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.5000
Epoch 20/100
357/357 - 160s - loss: 1.4924e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.7000 - val_loss: 0.0133 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.5682
Epoch 21/100
357/357 - 161s - loss: 1.6581e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.7368 - val_loss: 0.0098 - val_f1_score: 0.0049 - val_precision_4: 0.9355 - val_recall_4: 0.6591
Epoch 22/100
357/357 - 161s - loss: 6.4936e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.3000 - val_loss: 0.0186 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.3182
Epoch 23/100
357/357 - 161s - loss: 3.8300e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.4842 - val_loss: 0.0302 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.0682
Epoch 24/100
357/357 - 160s - loss: 2.9633e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.5474 - val_loss: 0.0100 - val_f1_score: 0.0049 - val_precision_4: 0.9062 - val_recall_4: 0.6591
Epoch 25/100
357/357 - 160s - loss: 9.3080e-06 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.9263 - val_loss: 0.0138 - val_f1_score: 0.0049 - val_precision_4: 0.9643 - val_recall_4: 0.6136
Epoch 26/100
357/357 - 161s - loss: 5.2325e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.2737 - val_loss: 0.0156 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.2955
Epoch 27/100
357/357 - 161s - loss: 2.0827e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.5211 - val_loss: 0.0174 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.3864
Epoch 28/100
357/357 - 160s - loss: 2.5324e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.5842 - val_loss: 0.0120 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.5227
Epoch 29/100
357/357 - 161s - loss: 9.2980e-06 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.8474 - val_loss: 0.0106 - val_f1_score: 0.0049 - val_precision_4: 0.9355 - val_recall_4: 0.6591
Epoch 30/100
357/357 - 161s - loss: 1.5026e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.7842 - val_loss: 0.0119 - val_f1_score: 0.0049 - val_precision_4: 0.9310 - val_recall_4: 0.6136
Epoch 31/100
357/357 - 160s - loss: 3.0585e-05 - f1_score: 0.0053 - precision_4: 0.9915 - recall_4: 0.6105 - val_loss: 0.0129 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.5227
Epoch 32/100
357/357 - 160s - loss: 1.1500e-05 - f1_score: 0.0053 - precision_4: 1.0000 - recall_4: 0.7579 - val_loss: 0.0117 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.6364
Epoch 33/100
357/357 - 161s - loss: 5.5806e-06 - f1_score: 0.0054 - precision_4: 1.0000 - recall_4: 0.8632 - val_loss: 0.0113 - val_f1_score: 0.0049 - val_precision_4: 0.9655 - val_recall_4: 0.6364
Epoch 34/100
357/357 - 160s - loss: 3.6722e-06 - f1_score: 0.0054 - precision_4: 1.0000 - recall_4: 0.8947 - val_loss: 0.0103 - val_f1_score: 0.0049 - val_precision_4: 0.9091 - val_recall_4: 0.6818
Epoch 35/100
357/357 - 160s - loss: 3.1085e-06 - f1_score: 0.0055 - precision_4: 1.0000 - recall_4: 0.9526 - val_loss: 0.0126 - val_f1_score: 0.0049 - val_precision_4: 0.9667 - val_recall_4: 0.6591
Epoch 36/100
357/357 - 160s - loss: 6.3128e-06 - f1_score: 0.0055 - precision_4: 1.0000 - recall_4: 0.8895 - val_loss: 0.0144 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.5682
Epoch 37/100
357/357 - 160s - loss: 4.3327e-06 - f1_score: 0.0056 - precision_4: 1.0000 - recall_4: 0.8947 - val_loss: 0.0105 - val_f1_score: 0.0049 - val_precision_4: 0.9394 - val_recall_4: 0.7045
Epoch 38/100
357/357 - 160s - loss: 3.8330e-06 - f1_score: 0.0056 - precision_4: 1.0000 - recall_4: 0.9421 - val_loss: 0.0113 - val_f1_score: 0.0049 - val_precision_4: 0.9394 - val_recall_4: 0.7045
Epoch 39/100
357/357 - 160s - loss: 2.0077e-05 - f1_score: 0.0054 - precision_4: 1.0000 - recall_4: 0.7947 - val_loss: 0.0119 - val_f1_score: 0.0049 - val_precision_4: 0.9333 - val_recall_4: 0.6364
Epoch 40/100
357/357 - 161s - loss: 7.2365e-05 - f1_score: 0.0053 - precision_4: 0.9841 - recall_4: 0.3263 - val_loss: 0.0195 - val_f1_score: 0.0049 - val_precision_4: 1.0000 - val_recall_4: 0.3409
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc2c22f70> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 1e-05,  dropout_rate = 0.8
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc0778310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0231 - f1_score: 0.0053 - precision_5: 0.0036 - recall_5: 0.0105 - val_loss: 0.0387 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 2/100
357/357 - 160s - loss: 0.0021 - f1_score: 0.0053 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0423 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 3/100
357/357 - 160s - loss: 0.0014 - f1_score: 0.0054 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0446 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 4/100
357/357 - 160s - loss: 0.0013 - f1_score: 0.0055 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0460 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 0.0012 - f1_score: 0.0053 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0472 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 6/100
357/357 - 161s - loss: 0.0012 - f1_score: 0.0053 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0482 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 7/100
357/357 - 161s - loss: 0.0011 - f1_score: 0.0052 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0492 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 8/100
357/357 - 161s - loss: 0.0011 - f1_score: 0.0053 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0501 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 9.5272e-04 - f1_score: 0.0052 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0510 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 10/100
357/357 - 160s - loss: 8.9751e-04 - f1_score: 0.0053 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0515 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 11/100
357/357 - 161s - loss: 8.7062e-04 - f1_score: 0.0052 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0519 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 12/100
357/357 - 161s - loss: 8.2911e-04 - f1_score: 0.0054 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0536 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 13/100
357/357 - 161s - loss: 9.2330e-04 - f1_score: 0.0053 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0539 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 14/100
357/357 - 160s - loss: 7.3257e-04 - f1_score: 0.0052 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0556 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 15/100
357/357 - 160s - loss: 7.5179e-04 - f1_score: 0.0054 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0562 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 16/100
357/357 - 160s - loss: 6.9300e-04 - f1_score: 0.0053 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0571 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 17/100
357/357 - 161s - loss: 6.8212e-04 - f1_score: 0.0057 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0583 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 18/100
357/357 - 160s - loss: 7.1839e-04 - f1_score: 0.0052 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0587 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 19/100
357/357 - 160s - loss: 6.4485e-04 - f1_score: 0.0051 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0596 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 20/100
357/357 - 160s - loss: 6.1842e-04 - f1_score: 0.0054 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0599 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 21/100
357/357 - 161s - loss: 6.3115e-04 - f1_score: 0.0054 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0609 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 22/100
357/357 - 161s - loss: 7.3289e-04 - f1_score: 0.0050 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0613 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 23/100
357/357 - 160s - loss: 5.7033e-04 - f1_score: 0.0056 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0628 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 24/100
357/357 - 160s - loss: 5.6005e-04 - f1_score: 0.0061 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0637 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 25/100
357/357 - 160s - loss: 5.9584e-04 - f1_score: 0.0049 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0634 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 26/100
357/357 - 160s - loss: 6.1062e-04 - f1_score: 0.0059 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0631 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 27/100
357/357 - 160s - loss: 6.1263e-04 - f1_score: 0.0061 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0648 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 28/100
357/357 - 160s - loss: 5.6753e-04 - f1_score: 0.0058 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0655 - val_f1_score: 0.0169 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Epoch 29/100
357/357 - 160s - loss: 5.6612e-04 - f1_score: 0.0056 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - val_loss: 0.0649 - val_f1_score: 0.0049 - val_precision_5: 0.0000e+00 - val_recall_5: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd96cd3f70> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.0001,  dropout_rate = 0.0
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd38c049310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0026 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0300 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 2/100
357/357 - 161s - loss: 8.8798e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0301 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 3/100
357/357 - 161s - loss: 7.9754e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0219 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 4/100
357/357 - 161s - loss: 8.6140e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0278 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 5/100
357/357 - 161s - loss: 9.0057e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0297 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 6/100
357/357 - 161s - loss: 8.3981e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0224 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 7/100
357/357 - 161s - loss: 7.5676e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0279 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 8/100
357/357 - 161s - loss: 8.2312e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0281 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 9/100
357/357 - 161s - loss: 8.9733e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0265 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 10/100
357/357 - 161s - loss: 8.2098e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0279 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 11/100
357/357 - 161s - loss: 7.0051e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0218 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 12/100
357/357 - 161s - loss: 5.9866e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0191 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 13/100
357/357 - 161s - loss: 4.6507e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0148 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 14/100
357/357 - 161s - loss: 4.8287e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0256 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 15/100
357/357 - 161s - loss: 5.1075e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0273 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 16/100
357/357 - 160s - loss: 1.5438e-04 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0318 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 17/100
357/357 - 161s - loss: 9.2218e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0300 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Epoch 18/100
357/357 - 161s - loss: 9.2200e-05 - f1_score: 0.0053 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 0.0301 - val_f1_score: 0.0049 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc30e2f70> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.0001,  dropout_rate = 0.4
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd4181b9550> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0027 - f1_score: 0.0053 - precision_7: 0.0115 - recall_7: 0.0053 - val_loss: 0.0363 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 2/100
357/357 - 160s - loss: 1.1288e-04 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0352 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 3/100
357/357 - 161s - loss: 1.0740e-04 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0359 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 4/100
357/357 - 160s - loss: 9.5713e-05 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0190 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 5/100
357/357 - 161s - loss: 9.8890e-05 - f1_score: 0.0053 - precision_7: 1.0000 - recall_7: 0.0158 - val_loss: 0.0361 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 6/100
357/357 - 161s - loss: 1.0815e-04 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0347 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 9.1077e-05 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0176 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 8/100
357/357 - 161s - loss: 7.6188e-05 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0206 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 8.4461e-05 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0357 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 10/100
357/357 - 161s - loss: 1.0862e-04 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0337 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 11/100
357/357 - 161s - loss: 8.6192e-05 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0354 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 12/100
357/357 - 161s - loss: 1.0804e-04 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0341 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Epoch 13/100
357/357 - 160s - loss: 1.0881e-04 - f1_score: 0.0053 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 0.0325 - val_f1_score: 0.0049 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc0731f70> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.0001,  dropout_rate = 0.8
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc2440e50> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0028 - f1_score: 0.0054 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0591 - val_f1_score: 0.0049 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 2/100
357/357 - 160s - loss: 4.7562e-04 - f1_score: 0.0052 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0620 - val_f1_score: 0.0049 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 3/100
357/357 - 160s - loss: 4.5846e-04 - f1_score: 0.0051 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0654 - val_f1_score: 0.0049 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 4/100
357/357 - 161s - loss: 5.1244e-04 - f1_score: 0.0052 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0660 - val_f1_score: 0.0049 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 4.9275e-04 - f1_score: 0.0058 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0707 - val_f1_score: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 4.4866e-04 - f1_score: 0.0058 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0705 - val_f1_score: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 4.9457e-04 - f1_score: 0.0054 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0692 - val_f1_score: 0.0267 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 8/100
357/357 - 160s - loss: 4.8367e-04 - f1_score: 0.0055 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0533 - val_f1_score: 0.0072 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 9/100
357/357 - 161s - loss: 4.1124e-04 - f1_score: 0.0073 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0427 - val_f1_score: 0.1315 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 10/100
357/357 - 161s - loss: 4.6197e-04 - f1_score: 0.0081 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0346 - val_f1_score: 0.0277 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 11/100
357/357 - 160s - loss: 3.9148e-04 - f1_score: 0.0091 - precision_8: 1.0000 - recall_8: 0.0053 - val_loss: 0.0407 - val_f1_score: 0.1326 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 12/100
357/357 - 161s - loss: 3.7100e-04 - f1_score: 0.0117 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0399 - val_f1_score: 0.2681 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 13/100
357/357 - 160s - loss: 3.5358e-04 - f1_score: 0.0154 - precision_8: 1.0000 - recall_8: 0.0211 - val_loss: 0.0589 - val_f1_score: 0.6316 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 14/100
357/357 - 161s - loss: 2.7842e-04 - f1_score: 0.0165 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0265 - val_f1_score: 0.3654 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 15/100
357/357 - 161s - loss: 2.8245e-04 - f1_score: 0.0214 - precision_8: 0.6000 - recall_8: 0.0158 - val_loss: 0.0508 - val_f1_score: 0.7778 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 16/100
357/357 - 161s - loss: 3.1906e-04 - f1_score: 0.0217 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0360 - val_f1_score: 0.6966 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 17/100
357/357 - 160s - loss: 2.9244e-04 - f1_score: 0.0226 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - val_loss: 0.0331 - val_f1_score: 0.5036 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 18/100
357/357 - 161s - loss: 3.2314e-04 - f1_score: 0.0280 - precision_8: 1.0000 - recall_8: 0.0211 - val_loss: 0.0274 - val_f1_score: 0.6733 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Epoch 19/100
357/357 - 161s - loss: 3.3476e-04 - f1_score: 0.0221 - precision_8: 1.0000 - recall_8: 0.0368 - val_loss: 0.0418 - val_f1_score: 0.6882 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc0501af0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.001,  dropout_rate = 0.0
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc28df3a0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0012 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0314 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Epoch 2/100
357/357 - 161s - loss: 9.3146e-05 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0317 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Epoch 3/100
357/357 - 161s - loss: 9.2417e-05 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0311 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Epoch 4/100
357/357 - 161s - loss: 9.2376e-05 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0311 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Epoch 5/100
357/357 - 161s - loss: 9.1983e-05 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0304 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Epoch 6/100
357/357 - 161s - loss: 9.2461e-05 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Epoch 7/100
357/357 - 161s - loss: 9.2428e-05 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0323 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Epoch 8/100
357/357 - 161s - loss: 9.2420e-05 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0314 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Epoch 9/100
357/357 - 161s - loss: 9.2475e-05 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0308 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Epoch 10/100
357/357 - 161s - loss: 9.2198e-05 - f1_score: 0.0053 - precision_9: 0.0000e+00 - recall_9: 0.0000e+00 - val_loss: 0.0315 - val_f1_score: 0.0049 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc0082ca0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.001,  dropout_rate = 0.4
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc2dd5c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0016 - f1_score: 0.0053 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00 - val_loss: 0.0390 - val_f1_score: 0.0049 - val_precision_10: 0.0000e+00 - val_recall_10: 0.0000e+00
Epoch 2/100
357/357 - 161s - loss: 1.1132e-04 - f1_score: 0.0053 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00 - val_loss: 0.0355 - val_f1_score: 0.0049 - val_precision_10: 0.0000e+00 - val_recall_10: 0.0000e+00
Epoch 3/100
357/357 - 160s - loss: 1.1022e-04 - f1_score: 0.0053 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00 - val_loss: 0.0376 - val_f1_score: 0.0049 - val_precision_10: 0.0000e+00 - val_recall_10: 0.0000e+00
Epoch 4/100
357/357 - 161s - loss: 1.0751e-04 - f1_score: 0.0053 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00 - val_loss: 0.0348 - val_f1_score: 0.0049 - val_precision_10: 0.0000e+00 - val_recall_10: 0.0000e+00
Epoch 5/100
357/357 - 161s - loss: 1.0866e-04 - f1_score: 0.0053 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00 - val_loss: 0.0322 - val_f1_score: 0.0049 - val_precision_10: 0.0000e+00 - val_recall_10: 0.0000e+00
Epoch 6/100
357/357 - 161s - loss: 1.0847e-04 - f1_score: 0.0053 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00 - val_loss: 0.0367 - val_f1_score: 0.0049 - val_precision_10: 0.0000e+00 - val_recall_10: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 1.0830e-04 - f1_score: 0.0053 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00 - val_loss: 0.0354 - val_f1_score: 0.0049 - val_precision_10: 0.0000e+00 - val_recall_10: 0.0000e+00
Epoch 8/100
357/357 - 160s - loss: 1.0940e-04 - f1_score: 0.0053 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00 - val_loss: 0.0346 - val_f1_score: 0.0049 - val_precision_10: 0.0000e+00 - val_recall_10: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 1.0787e-04 - f1_score: 0.0053 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00 - val_loss: 0.0356 - val_f1_score: 0.0049 - val_precision_10: 0.0000e+00 - val_recall_10: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc2ed1c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.001,  dropout_rate = 0.8
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcd97636700> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0040 - f1_score: 0.0052 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_loss: 0.0694 - val_f1_score: 0.0000e+00 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00
Epoch 2/100
357/357 - 161s - loss: 6.4762e-04 - f1_score: 0.0051 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_loss: 0.0752 - val_f1_score: 0.0000e+00 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00
Epoch 3/100
357/357 - 161s - loss: 4.8408e-04 - f1_score: 0.0048 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_loss: 0.0779 - val_f1_score: 0.0000e+00 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00
Epoch 4/100
357/357 - 161s - loss: 6.3597e-04 - f1_score: 0.0057 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_loss: 0.0794 - val_f1_score: 0.0000e+00 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00
Epoch 5/100
357/357 - 161s - loss: 6.6105e-04 - f1_score: 0.0054 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_loss: 0.0801 - val_f1_score: 0.0000e+00 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 6.1080e-04 - f1_score: 0.0053 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_loss: 0.0827 - val_f1_score: 0.0000e+00 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 5.5712e-04 - f1_score: 0.0047 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_loss: 0.0867 - val_f1_score: 0.0000e+00 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00
Epoch 8/100
357/357 - 161s - loss: 5.2969e-04 - f1_score: 0.0053 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_loss: 0.0809 - val_f1_score: 0.0000e+00 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc2ea1c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.01,  dropout_rate = 0.0
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc06d5700> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0143 - f1_score: 0.0034 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.1255 - val_f1_score: 0.0000e+00 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 2/100
357/357 - 161s - loss: 2.0514e-04 - f1_score: 0.0051 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0307 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 3/100
357/357 - 161s - loss: 9.3470e-05 - f1_score: 0.0053 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0321 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 4/100
357/357 - 160s - loss: 9.3540e-05 - f1_score: 0.0053 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0326 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 9.3221e-05 - f1_score: 0.0053 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0335 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 9.3295e-05 - f1_score: 0.0053 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0310 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 9.2664e-05 - f1_score: 0.0053 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0310 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 8/100
357/357 - 160s - loss: 9.3344e-05 - f1_score: 0.0053 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0314 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 9.3397e-05 - f1_score: 0.0053 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0334 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 10/100
357/357 - 160s - loss: 9.3441e-05 - f1_score: 0.0053 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0349 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 11/100
357/357 - 161s - loss: 1.4164e-04 - f1_score: 0.0055 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0329 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Epoch 12/100
357/357 - 160s - loss: 9.2861e-05 - f1_score: 0.0053 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00 - val_loss: 0.0339 - val_f1_score: 0.0049 - val_precision_12: 0.0000e+00 - val_recall_12: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc2e47c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.01,  dropout_rate = 0.4
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_13/bert/pooler/dense/kernel:0', 'tf_bert_model_13/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_13/bert/pooler/dense/kernel:0', 'tf_bert_model_13/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc3107700> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 168s - loss: 0.0321 - f1_score: 0.0057 - precision_13: 0.0059 - recall_13: 0.0105 - val_loss: 0.1055 - val_f1_score: 0.0000e+00 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 2/100
357/357 - 161s - loss: 2.5485e-04 - f1_score: 0.0054 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0772 - val_f1_score: 0.0000e+00 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 3/100
357/357 - 160s - loss: 2.1196e-04 - f1_score: 0.0048 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0676 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 4/100
357/357 - 161s - loss: 1.8554e-04 - f1_score: 0.0056 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0588 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 1.6186e-04 - f1_score: 0.0057 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0517 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 1.5729e-04 - f1_score: 0.0052 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0496 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 1.5033e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0479 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 8/100
357/357 - 160s - loss: 1.4179e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0439 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 1.3458e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0426 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 10/100
357/357 - 160s - loss: 1.3369e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0386 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 11/100
357/357 - 160s - loss: 1.3187e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0425 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 12/100
357/357 - 159s - loss: 1.3739e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0422 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 13/100
357/357 - 160s - loss: 1.2927e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0410 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 14/100
357/357 - 160s - loss: 1.3094e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0401 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 15/100
357/357 - 160s - loss: 1.3539e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0429 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 16/100
357/357 - 161s - loss: 1.2778e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0468 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 17/100
357/357 - 160s - loss: 1.3282e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0448 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 18/100
357/357 - 160s - loss: 1.2670e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0395 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 19/100
357/357 - 160s - loss: 1.3412e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0474 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 20/100
357/357 - 160s - loss: 1.4660e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0412 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 21/100
357/357 - 160s - loss: 1.3516e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0423 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 22/100
357/357 - 159s - loss: 1.3314e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0358 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Epoch 23/100
357/357 - 159s - loss: 1.3789e-04 - f1_score: 0.0053 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00 - val_loss: 0.0521 - val_f1_score: 0.0049 - val_precision_13: 0.0000e+00 - val_recall_13: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc2d03c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.01,  dropout_rate = 0.8
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_14/bert/pooler/dense/kernel:0', 'tf_bert_model_14/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_14/bert/pooler/dense/kernel:0', 'tf_bert_model_14/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc0510940> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 167s - loss: 0.0481 - f1_score: 0.0048 - precision_14: 0.0023 - recall_14: 0.0053 - val_loss: 0.2128 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 2/100
357/357 - 160s - loss: 0.0020 - f1_score: 0.0066 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.1517 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 3/100
357/357 - 160s - loss: 0.0017 - f1_score: 0.0042 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.1140 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 4/100
357/357 - 160s - loss: 0.0015 - f1_score: 0.0046 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0930 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 5/100
357/357 - 161s - loss: 0.0014 - f1_score: 0.0048 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0817 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 0.0012 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0775 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 0.0012 - f1_score: 0.0051 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0709 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 8/100
357/357 - 160s - loss: 9.1110e-04 - f1_score: 0.0048 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0746 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 9/100
357/357 - 161s - loss: 8.1556e-04 - f1_score: 0.0052 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0772 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 10/100
357/357 - 161s - loss: 6.7822e-04 - f1_score: 0.0058 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0851 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 11/100
357/357 - 161s - loss: 6.2322e-04 - f1_score: 0.0057 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0658 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 12/100
357/357 - 160s - loss: 5.3040e-04 - f1_score: 0.0054 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0693 - val_f1_score: 0.0000e+00 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 13/100
357/357 - 160s - loss: 4.6507e-04 - f1_score: 0.0049 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0640 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 14/100
357/357 - 160s - loss: 4.0432e-04 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0653 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 15/100
357/357 - 160s - loss: 3.7883e-04 - f1_score: 0.0054 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0596 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 16/100
357/357 - 160s - loss: 3.3687e-04 - f1_score: 0.0051 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0602 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 17/100
357/357 - 160s - loss: 2.9539e-04 - f1_score: 0.0052 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0538 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 18/100
357/357 - 160s - loss: 2.6655e-04 - f1_score: 0.0054 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0491 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 19/100
357/357 - 160s - loss: 2.4137e-04 - f1_score: 0.0055 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0453 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 20/100
357/357 - 160s - loss: 2.4157e-04 - f1_score: 0.0054 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0443 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 21/100
357/357 - 160s - loss: 2.1252e-04 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0590 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 22/100
357/357 - 160s - loss: 1.9521e-04 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0413 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 23/100
357/357 - 160s - loss: 1.8998e-04 - f1_score: 0.0050 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0393 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 24/100
357/357 - 160s - loss: 1.8602e-04 - f1_score: 0.0052 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0471 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 25/100
357/357 - 160s - loss: 1.9464e-04 - f1_score: 0.0052 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0385 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 26/100
357/357 - 160s - loss: 1.7130e-04 - f1_score: 0.0052 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0398 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 27/100
357/357 - 160s - loss: 1.5738e-04 - f1_score: 0.0054 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0354 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 28/100
357/357 - 160s - loss: 1.4183e-04 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0359 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 29/100
357/357 - 160s - loss: 1.2961e-04 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0343 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 30/100
357/357 - 160s - loss: 1.2343e-04 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0347 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 31/100
357/357 - 160s - loss: 1.1662e-04 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0345 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 32/100
357/357 - 160s - loss: 1.1033e-04 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0330 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 33/100
357/357 - 160s - loss: 1.0891e-04 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0318 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 34/100
357/357 - 160s - loss: 9.9042e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0322 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 35/100
357/357 - 161s - loss: 9.9727e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0314 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 36/100
357/357 - 160s - loss: 9.6973e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0297 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 37/100
357/357 - 160s - loss: 9.6015e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0298 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 38/100
357/357 - 160s - loss: 9.6671e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0284 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 39/100
357/357 - 160s - loss: 9.5293e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0290 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 40/100
357/357 - 160s - loss: 9.4604e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0296 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 41/100
357/357 - 160s - loss: 9.4378e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0274 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 42/100
357/357 - 160s - loss: 9.4062e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 43/100
357/357 - 161s - loss: 9.9258e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0272 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 44/100
357/357 - 160s - loss: 9.3602e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0284 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 45/100
357/357 - 160s - loss: 9.1706e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0290 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 46/100
357/357 - 160s - loss: 9.1497e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 47/100
357/357 - 160s - loss: 9.1497e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 48/100
357/357 - 160s - loss: 9.1506e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 49/100
357/357 - 161s - loss: 9.1507e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 50/100
357/357 - 160s - loss: 9.1511e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 51/100
357/357 - 160s - loss: 9.1495e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0294 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 52/100
357/357 - 160s - loss: 9.1507e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 53/100
357/357 - 161s - loss: 9.1504e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0294 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 54/100
357/357 - 160s - loss: 9.1508e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 55/100
357/357 - 161s - loss: 9.1516e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Epoch 56/100
357/357 - 160s - loss: 9.1509e-05 - f1_score: 0.0053 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00 - val_loss: 0.0294 - val_f1_score: 0.0049 - val_precision_14: 0.0000e+00 - val_recall_14: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdc023fc10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.1,  dropout_rate = 0.0
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_15/bert/pooler/dense/kernel:0', 'tf_bert_model_15/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_15/bert/pooler/dense/kernel:0', 'tf_bert_model_15/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc026b4c0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 167s - loss: 0.3325 - f1_score: 0.0057 - precision_15: 0.0025 - recall_15: 0.0053 - val_loss: 0.0267 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 2/100
357/357 - 160s - loss: 1.4944e-04 - f1_score: 0.0054 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0256 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 3/100
357/357 - 159s - loss: 1.2373e-04 - f1_score: 0.0052 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0229 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 4/100
357/357 - 160s - loss: 1.0404e-04 - f1_score: 0.0053 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0256 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 1.1248e-04 - f1_score: 0.0052 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0236 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 6/100
357/357 - 160s - loss: 1.0184e-04 - f1_score: 0.0053 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0206 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 1.0233e-04 - f1_score: 0.0053 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0249 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 8/100
357/357 - 159s - loss: 1.0106e-04 - f1_score: 0.0053 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0229 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 1.0599e-04 - f1_score: 0.0053 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0343 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 10/100
357/357 - 159s - loss: 1.0590e-04 - f1_score: 0.0053 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0223 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 11/100
357/357 - 160s - loss: 1.1510e-04 - f1_score: 0.0053 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0385 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 12/100
357/357 - 160s - loss: 1.2636e-04 - f1_score: 0.0053 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0193 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Epoch 13/100
357/357 - 159s - loss: 1.0862e-04 - f1_score: 0.0053 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00 - val_loss: 0.0323 - val_f1_score: 0.0049 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd96bcac10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.1,  dropout_rate = 0.4
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_16/bert/pooler/dense/kernel:0', 'tf_bert_model_16/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_16/bert/pooler/dense/kernel:0', 'tf_bert_model_16/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcd96c75c10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 167s - loss: 0.1245 - f1_score: 0.0093 - precision_16: 0.0054 - recall_16: 0.0105 - val_loss: 1.3354 - val_f1_score: 0.0000e+00 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 2/100
357/357 - 159s - loss: 0.0082 - f1_score: 0.0107 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 1.2059 - val_f1_score: 0.0000e+00 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 3/100
357/357 - 160s - loss: 0.0018 - f1_score: 0.0027 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.3611 - val_f1_score: 0.0000e+00 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 4/100
357/357 - 159s - loss: 8.7128e-04 - f1_score: 0.0042 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.2081 - val_f1_score: 0.0000e+00 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 6.3053e-04 - f1_score: 0.0046 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.1409 - val_f1_score: 0.0000e+00 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 6/100
357/357 - 159s - loss: 3.6844e-04 - f1_score: 0.0046 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0954 - val_f1_score: 0.0000e+00 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 2.7162e-04 - f1_score: 0.0058 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0797 - val_f1_score: 0.0000e+00 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 8/100
357/357 - 160s - loss: 2.4386e-04 - f1_score: 0.0056 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0656 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 2.0003e-04 - f1_score: 0.0054 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0478 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 10/100
357/357 - 160s - loss: 1.9566e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0504 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 11/100
357/357 - 159s - loss: 1.7838e-04 - f1_score: 0.0052 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0487 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 12/100
357/357 - 159s - loss: 1.7154e-04 - f1_score: 0.0054 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0443 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 13/100
357/357 - 159s - loss: 1.5175e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0421 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 14/100
357/357 - 160s - loss: 1.5001e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0423 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 15/100
357/357 - 160s - loss: 1.4440e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0439 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 16/100
357/357 - 160s - loss: 1.3503e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0374 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 17/100
357/357 - 160s - loss: 1.2737e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0440 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 18/100
357/357 - 159s - loss: 1.2878e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0344 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 19/100
357/357 - 160s - loss: 1.2485e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0389 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 20/100
357/357 - 160s - loss: 1.1635e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0439 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 21/100
357/357 - 160s - loss: 1.1648e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0323 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 22/100
357/357 - 160s - loss: 1.1433e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0413 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 23/100
357/357 - 160s - loss: 1.1146e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0374 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 24/100
357/357 - 160s - loss: 1.0900e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0323 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 25/100
357/357 - 160s - loss: 1.0602e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0362 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 26/100
357/357 - 160s - loss: 1.0636e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0404 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 27/100
357/357 - 160s - loss: 1.0172e-04 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0335 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 28/100
357/357 - 160s - loss: 9.6191e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0305 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 29/100
357/357 - 160s - loss: 9.2588e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0288 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 30/100
357/357 - 160s - loss: 9.1668e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0580 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 31/100
357/357 - 160s - loss: 9.2591e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 32/100
357/357 - 160s - loss: 9.1523e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 33/100
357/357 - 160s - loss: 9.1505e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0291 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 34/100
357/357 - 159s - loss: 9.1503e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 35/100
357/357 - 160s - loss: 9.1529e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0295 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 36/100
357/357 - 159s - loss: 9.1529e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 37/100
357/357 - 159s - loss: 9.1569e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0294 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 38/100
357/357 - 160s - loss: 9.1527e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Epoch 39/100
357/357 - 160s - loss: 9.1591e-05 - f1_score: 0.0053 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_loss: 0.0295 - val_f1_score: 0.0049 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd44809dc10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Classifier with lr = 0.1,  dropout_rate = 0.8
Epoch 1/100
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_17/bert/pooler/dense/kernel:0', 'tf_bert_model_17/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_17/bert/pooler/dense/kernel:0', 'tf_bert_model_17/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fcdc0801700> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
357/357 - 167s - loss: 0.0867 - f1_score: 0.0057 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 1.1615 - val_f1_score: 0.0000e+00 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 2/100
357/357 - 159s - loss: 0.0051 - f1_score: 0.0052 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.3798 - val_f1_score: 0.0000e+00 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 3/100
357/357 - 160s - loss: 0.0019 - f1_score: 0.0065 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.1879 - val_f1_score: 0.0000e+00 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 4/100
357/357 - 160s - loss: 7.6914e-04 - f1_score: 0.0051 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0265 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 5/100
357/357 - 160s - loss: 4.3957e-04 - f1_score: 0.0054 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0381 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 6/100
357/357 - 159s - loss: 2.8705e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0359 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 7/100
357/357 - 160s - loss: 2.2267e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0257 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 8/100
357/357 - 159s - loss: 1.7672e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0269 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 9/100
357/357 - 160s - loss: 1.4868e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0263 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 10/100
357/357 - 160s - loss: 1.2875e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0286 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 11/100
357/357 - 160s - loss: 1.2427e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0262 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 12/100
357/357 - 159s - loss: 1.1494e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0277 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 13/100
357/357 - 160s - loss: 1.0980e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0262 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 14/100
357/357 - 160s - loss: 1.0336e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0264 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 15/100
357/357 - 160s - loss: 1.0048e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0265 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 16/100
357/357 - 160s - loss: 9.6896e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0270 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 17/100
357/357 - 159s - loss: 9.6449e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0275 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 18/100
357/357 - 160s - loss: 9.3318e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0279 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 19/100
357/357 - 160s - loss: 9.2499e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0282 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 20/100
357/357 - 159s - loss: 9.1942e-05 - f1_score: 0.0054 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0285 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 21/100
357/357 - 160s - loss: 9.1911e-05 - f1_score: 0.0054 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0287 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 22/100
357/357 - 160s - loss: 9.1614e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0289 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 23/100
357/357 - 159s - loss: 1.1403e-04 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0290 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 24/100
357/357 - 160s - loss: 9.2722e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0291 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 25/100
357/357 - 159s - loss: 9.1495e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0292 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 26/100
357/357 - 161s - loss: 9.1488e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 27/100
357/357 - 162s - loss: 9.1492e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 28/100
357/357 - 165s - loss: 9.1492e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 29/100
357/357 - 163s - loss: 1.0025e-04 - f1_score: 0.0054 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0313 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 30/100
357/357 - 162s - loss: 9.2978e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0293 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
Epoch 31/100
357/357 - 160s - loss: 9.1502e-05 - f1_score: 0.0053 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 0.0294 - val_f1_score: 0.0049 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00
